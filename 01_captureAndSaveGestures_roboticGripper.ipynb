{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROBOTIC GRIPPER\n",
    "\n",
    "## A Robotic Gripper Operated by Gestures Learned Trough DeepLearning\n",
    "\n",
    "## Phase 1 Implementation: Capturing and saving gestures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project allows a user to control a robotic gripper using gestures captured by a webcam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - How does it works\n",
    "\n",
    "The project is diveded in 3 main phases, in order to fulfill user requests:\n",
    "\n",
    "- Phase 1: Images must be captured from the webcam to compound a labeled gestures dataset.\n",
    "  The dataset will feed trainning and testing datasets to be used in supervised learning.\n",
    "    \n",
    "- Phase 2: A deep learning model, basically a neural network, will be created and used to train the gestures recognition, using keras and tensorflow.\n",
    "    \n",
    "- Phase 3: A program will be used to sequentially capture webcam images.\n",
    "  The images will be classifyed by the model trainned in Phase 2, and the result will be used to operate the robotic gripper.\n",
    "  \n",
    "**This notebook implements the phase 1 of the project, there are two other notebooks to be executed after this one.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Capturing labeled gestures images\n",
    "\n",
    "Images will be captured from the webcam.\n",
    "A folder named **capture** will have several subfolders.\n",
    "The subfolders will have meaningful names, such as **left**, **right**, and so on.\n",
    "The subfolder named **left** will hold images of teh gesture that yields the command **turn to the left**.\n",
    "This is so that later the subfolders name will become the ground truth values of the datasets for the machine learning process.\n",
    "\n",
    "For controlling the robotic gripper, we are going to use nine commands:\n",
    "    1. nothing\n",
    "    2. left\n",
    "    3. right\n",
    "    4. up\n",
    "    5. down\n",
    "    6. foward\n",
    "    7. back\n",
    "    8. grip\n",
    "    9. loose\n",
    "    \n",
    "Some examples of images are:\n",
    "\n",
    "<TABLE>\n",
    "    <CENTER>\n",
    "    <TR>\n",
    "      <TD>\n",
    "          <img src=\"images/nothing.jpg\" width=\"128\" height=\"96\" />\n",
    "      </TD>\n",
    "      <TD>\n",
    "          <img src=\"images/left.jpg\" width=\"128\" height=\"96\" />\n",
    "      </TD>\n",
    "      <TD>\n",
    "          <img src=\"images/right.jpg\" width=\"128\" height=\"96\" />\n",
    "      </TD>\n",
    "      <TD>\n",
    "          <img src=\"images/grip.jpg\" width=\"128\" height=\"96\" />\n",
    "      </TD>\n",
    "      <TD>\n",
    "          <img src=\"images/loose.jpg\" width=\"128\" height=\"96\" />\n",
    "      </TD>\n",
    "    </TR>\n",
    "    <TR>\n",
    "      <TD>\n",
    "          nothing\n",
    "      </TD>\n",
    "      <TD>\n",
    "          left\n",
    "      </TD>\n",
    "      <TD>\n",
    "          right\n",
    "      </TD>\n",
    "      <TD>\n",
    "          grip\n",
    "      </TD>\n",
    "      <TD>\n",
    "          loose\n",
    "      </TD>\n",
    "    </TR>\n",
    "    <TR>\n",
    "      <TD>\n",
    "          <img src=\"images/foward.jpg\" width=\"128\" height=\"96\" />\n",
    "      </TD>\n",
    "      <TD>\n",
    "          <img src=\"images/back.jpg\" width=\"128\" height=\"96\" />\n",
    "      </TD>\n",
    "      <TD>\n",
    "          <img src=\"images/up.jpg\" width=\"128\" height=\"96\" />\n",
    "      </TD>\n",
    "      <TD>\n",
    "          <img src=\"images/down.jpg\" width=\"128\" height=\"96\" />\n",
    "      </TD>\n",
    "    </TR>\n",
    "    <TR>\n",
    "      <TD>\n",
    "          foward\n",
    "      </TD>\n",
    "      <TD>\n",
    "          back\n",
    "      </TD>\n",
    "      <TD>\n",
    "          up\n",
    "      </TD>\n",
    "      <TD>\n",
    "          down\n",
    "      </TD>\n",
    "    </TR>\n",
    "    </CENTER>\n",
    "</TABLE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline \n",
    "import cv2\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **start_webcam_capture** receive tow parameters, **path** and **number_of_captures** (defaults to 10).\n",
    "  -  **path**: states where in your media the captured images will be stored. The trick is to make the last folder in the path the label of the gesture, i.e., path = 'capture/left', where the subfolder left holds images for gestures to be labeled left.\n",
    "  -  **number_of_captures**: is the number of images to be saved. One can interrupt the notebook kernel as a means to stop the running code of the actual notebook cell, but it sometimes damages the overall memory of the notebook, and then, one must go trough the menu> Kernel> Restart & Clear Output, and then run all cells form the top again. The before captured gesture images remain saved, and as timestamps are used in their name composition, they will not be overwritten. As a matter of fact, this function can be called over again until you have the amount of images you wish.\n",
    "  -  **return**: none. But, in the end the function informs how many images there are now on the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    function  start_webcam_capture\n",
    "    parameters:\n",
    "    path - the path to save captured gesture images files\n",
    "\"\"\"\n",
    "def start_webcam_capture(path, number_of_captures=10):\n",
    "    # variables to define play warning sound\n",
    "    frequency = 100 # Hertz\n",
    "    duration  = 50 # milliseconds\n",
    "    #lets make sure the path exists!\n",
    "    if not os.access(path, os.F_OK):\n",
    "        os.makedirs(path)\n",
    "    count_captures = 0\n",
    "    #using webcam 0.\n",
    "    #in some systems webcam may be under different numbers, i.e, 1 or 2 or 3 ...\n",
    "    vid = cv2.VideoCapture(0)\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        while(count_captures<number_of_captures):\n",
    "            # Capture frame-by-frame\n",
    "            ret, frame = vid.read()\n",
    "            if not ret:\n",
    "                # Release the Video Device if ret is false\n",
    "                vid.release()\n",
    "                # Message to be displayed after releasing the device\n",
    "                print(\"Released Video Resource due to capture fail!\")\n",
    "                break\n",
    "            # Convert the image from OpenCV BGR format to matplotlib RGB format\n",
    "            # to display the image\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            # check if it is time to save frame to a file\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if elapsed_time > 4:\n",
    "                # make sound to indicate action\n",
    "                os.system('play -n synth %s sin %s' % (duration/1000, frequency))\n",
    "                timestamp = datetime.utcnow().strftime('%Y_%m_%d_%H_%M_%S_%f')[:-3]\n",
    "                timestamp = timestamp + '.jpg'\n",
    "                image_filename = os.path.join(path, timestamp)\n",
    "                #print(image_filename)\n",
    "                cv2.imwrite(image_filename, frame)\n",
    "                #increment count_captures\n",
    "                count_captures += 1\n",
    "                #restart the timer\n",
    "                start_time = time.time()\n",
    "            # check for ESC\n",
    "            key = np.int16(cv2.waitKey(1))\n",
    "            if key == 27:\n",
    "                print(\"Esc key interrupted!\")\n",
    "                break  # esc to quit\n",
    "            # Turn off the axis\n",
    "            axis('off')\n",
    "            # Title of the window\n",
    "            title(\"Robotic Gripper Gestures Capture\")\n",
    "            # Display the frame\n",
    "            imshow(frame)\n",
    "            show()\n",
    "            # Display the frame until new frame is available\n",
    "            clear_output(wait=True)\n",
    "    except KeyboardInterrupt:\n",
    "        # Message to be displayed after releasing the device\n",
    "        print(\"keyboard interrupted!\")\n",
    "    # Release the Video Device\n",
    "    vid.release()\n",
    "    print(\"Released Video Resource\")\n",
    "    path, dirs, files = os.walk(path).__next__()\n",
    "    file_count = len(files)\n",
    "    print('There are now ', file_count, ' images in ', path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by capturing the gesture for **nothing**.\n",
    "Let's call the function with the desired parameters values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'capture/nothing'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **left**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'capture/left'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **right**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'capture/right'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **up**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'capture/up'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **down**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'capture/down'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **foward**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'capture/foward'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **back**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'capture/back'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **grip**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'capture/grip'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **loose**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'capture/loose'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of this notebook.\n",
    "Now you should have your capture path with 9 subfolders (nothing, left, right, up, down, foward, back, grip and loose), each one with several images.\n",
    "Next, we want to build a model and train it to recognize the gestures using de acquired data.\n",
    "This will be the task of the next notebook, **02_trainModel_roboticGripper**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Duodecimo, 2017, Dezember."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
