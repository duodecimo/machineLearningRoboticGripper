{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SANDBOX (ROBOTIC GRIPPER)\n",
    "\n",
    "## Here is the place to test stuff, etc.\n",
    "## Is always under construction and the code here don't deserve any credibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Capturing labeled gestures images\n",
    "\n",
    "Images will be captured from the webcam.\n",
    "A folder named **capture** will have several subfolders.\n",
    "The subfolders will have meaningful names, such as **left**, **right**, and so on.\n",
    "The subfolder named **left** will hold images of teh gesture that yields the command **turn to the left**.\n",
    "This is so that later the subfolders name will become the ground truth values of the datasets for the machine learning process.\n",
    "\n",
    "For controlling the robotic gripper, we are going to use nine commands:\n",
    "    1. nothing\n",
    "    2. left\n",
    "    3. right\n",
    "    4. up\n",
    "    5. down\n",
    "    6. foward\n",
    "    7. back\n",
    "    8. grip\n",
    "    9. loose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline \n",
    "import cv2\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    function  start_webcam_capture\n",
    "    parameters:\n",
    "    path - the path to save captured gesture images files\n",
    "\"\"\"\n",
    "def start_webcam_capture(path, number_of_captures=10):\n",
    "    # variables to define play warning sound\n",
    "    frequency = 100 # Hertz\n",
    "    duration  = 50 # milliseconds\n",
    "    #lets make sure the path exists!\n",
    "    if not os.access(path, os.F_OK):\n",
    "        os.makedirs(path)\n",
    "    count_captures = 0\n",
    "    #using webcam 0.\n",
    "    #in some systems webcam may be under different numbers, i.e, 1 or 2 or 3 ...\n",
    "    vid = cv2.VideoCapture(0)\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        while(count_captures<number_of_captures):\n",
    "            # Capture frame-by-frame\n",
    "            ret, frame = vid.read()\n",
    "            if not ret:\n",
    "                # Release the Video Device if ret is false\n",
    "                vid.release()\n",
    "                # Message to be displayed after releasing the device\n",
    "                print(\"Released Video Resource due to capture fail!\")\n",
    "                break\n",
    "            # Convert the image from OpenCV BGR format to matplotlib RGB format\n",
    "            # to display the image\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            # check if it is time to save frame to a file\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if elapsed_time > 4:\n",
    "                # make sound to indicate action\n",
    "                os.system('play -n synth %s sin %s' % (duration/1000, frequency))\n",
    "                timestamp = datetime.utcnow().strftime('%Y_%m_%d_%H_%M_%S_%f')[:-3]\n",
    "                timestamp = timestamp + '.jpg'\n",
    "                image_filename = os.path.join(path, timestamp)\n",
    "                #print(image_filename)\n",
    "                cv2.imwrite(image_filename, frame)\n",
    "                #increment count_captures\n",
    "                count_captures += 1\n",
    "                #restart the timer\n",
    "                start_time = time.time()\n",
    "            # check for ESC\n",
    "            key = np.int16(cv2.waitKey(1))\n",
    "            if key == 27:\n",
    "                print(\"Esc key interrupted!\")\n",
    "                break  # esc to quit\n",
    "            # Turn off the axis\n",
    "            axis('off')\n",
    "            # Title of the window\n",
    "            title(\"Robotic Gripper Gestures Capture\")\n",
    "            # Display the frame\n",
    "            imshow(frame)\n",
    "            show()\n",
    "            # Display the frame until new frame is available\n",
    "            clear_output(wait=True)\n",
    "    except KeyboardInterrupt:\n",
    "        # Message to be displayed after releasing the device\n",
    "        print(\"keyboard interrupted!\")\n",
    "    # Release the Video Device\n",
    "    vid.release()\n",
    "    print(\"Released Video Resource\")\n",
    "    path, dirs, files = os.walk(path).__next__()\n",
    "    file_count = len(files)\n",
    "    print('There are now ', file_count, ' images in ', path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by capturing the gesture for **nothing**.\n",
    "When you are done, select **Kernel** on jupyter notebook menu and then select **Interrupt**\n",
    "As the file names are bases on a complete and unique timestamp, if you wish, you can run the same code again to add more gestures images. You can even visually select and remove some files (in case of a mistake) using a external file manager from your operating system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'sandboxCapture/nothing'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **left**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'sandboxCapture/left'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **right**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'sandboxCapture/right'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **up**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'sandboxCapture/up'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **down**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'sandboxCapture/down'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **foward**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'sandboxCapture/foward'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **back**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'sandboxCapture/back'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **grip**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'sandboxCapture/grip'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **loose**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'sandboxCapture/loose'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Build the Model and train it using the captured gestures from the first phase\n",
    "\n",
    "We are going to build our [deep learning](https://en.wikipedia.org/wiki/Deep_learning) robotic gripper gesture commands model using [Keras](https://keras.io/) and [TensorFlow](https://www.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Lambda, Conv2D, MaxPooling2D, Dropout, Dense, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sandboxUtils import INPUT_SHAPE, batch_generator\n",
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **load_images_from_path** is auxiliary to the function **load_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    load_images_from_path\n",
    "'''\n",
    "def load_images_from_path(path, result, images, results):\n",
    "    for filename in os.listdir(path):\n",
    "      img = os.path.join(path,filename)\n",
    "      if img is not None:\n",
    "        images.append(img)\n",
    "        results.append(result)\n",
    "    return images, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "  images = []\n",
    "  results =[]\n",
    "  labels = ['nothing', 'left', 'right', 'grip', 'loose', 'foward', 'back', 'up', 'down']\n",
    "\n",
    "  #load a list of images and a corresponding list of results (images=640x480)\n",
    "  images, results = load_images_from_path('sandboxCapture/nothing/', 0, images, results)\n",
    "  images, results = load_images_from_path('sandboxCapture/left/', 1, images, results)\n",
    "  images, results = load_images_from_path('sandboxCapture/right/', 2, images, results)\n",
    "  images, results = load_images_from_path('sandboxCapture/grip/', 3, images, results)\n",
    "  images, results = load_images_from_path('sandboxCapture/loose/', 4, images, results)\n",
    "  images, results = load_images_from_path('sandboxCapture/foward/', 5, images, results)\n",
    "  images, results = load_images_from_path('sandboxCapture/back/', 6, images, results)\n",
    "  images, results = load_images_from_path('sandboxCapture/up/', 7, images, results)\n",
    "  images, results = load_images_from_path('sandboxCapture/down/', 8, images, results)\n",
    "\n",
    "  X_train, X_valid, y_train, y_valid = train_test_split(images, results, test_size=0.2, shuffle = True, random_state=0)\n",
    "\n",
    "  return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images:  79\n",
      "Valid Images:  20\n",
      "Train Results:  79\n",
      "Valid Results:  20\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = load_data()\n",
    "\n",
    "print(\"Train Images: \", len(X_train))\n",
    "print(\"Valid Images: \", len(X_valid))\n",
    "print(\"Train Results: \", len(y_train))\n",
    "print(\"Valid Results: \", len(y_valid))\n",
    "\n",
    "# if we wish to check some of the images, just change de index value\n",
    "# note that the index can't be bigger than the number of images -1\n",
    "#cv2.imshow('Capture', cv2.imread(X_train[80]))\n",
    "#print(X_train[80])\n",
    "#print(labels[results[80]])\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()\n",
    "#sys.exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(keep_prob):\n",
    "    \"\"\"\n",
    "    Modified NVIDIA model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: x/127.5-1.0, input_shape=INPUT_SHAPE))\n",
    "    model.add(Conv2D(24, 5, 5, activation='elu', subsample=(2, 2)))\n",
    "    model.add(Conv2D(36, 5, 5, activation='elu', subsample=(2, 2)))\n",
    "    model.add(Conv2D(48, 5, 5, activation='elu', subsample=(2, 2)))\n",
    "    model.add(Conv2D(64, 3, 3, activation='elu'))\n",
    "    model.add(Conv2D(64, 3, 3, activation='elu'))\n",
    "    model.add(Dropout(keep_prob))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='elu'))\n",
    "    model.add(Dense(50, activation='elu'))\n",
    "    model.add(Dense(10, activation='elu'))\n",
    "    # let's change from Dense(1) to Dense(9, activation='softmax')\n",
    "    # where 9 is the number of classes and softmax can be understood\n",
    "    #here: https://en.wikipedia.org/wiki/Softmax_function\n",
    "    # model.add(Dense(1))\n",
    "    model.add(Dense(9, activation='softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_3 (Lambda)                (None, 120, 160, 3)   0           lambda_input_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_11 (Convolution2D) (None, 58, 78, 24)    1824        lambda_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_12 (Convolution2D) (None, 27, 37, 36)    21636       convolution2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_13 (Convolution2D) (None, 12, 17, 48)    43248       convolution2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_14 (Convolution2D) (None, 10, 15, 64)    27712       convolution2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_15 (Convolution2D) (None, 8, 13, 64)     36928       convolution2d_14[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 8, 13, 64)     0           convolution2d_15[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 6656)          0           dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 100)           665700      flatten_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 50)            5050        dense_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 10)            510         dense_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 9)             99          dense_11[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 802,707\n",
      "Trainable params: 802,707\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keep_prob = 0.5\n",
    "model = build_model(keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, psave_best_only, learning_rate, samples_per_epoch, nb_epoch, batch_size, X_train, X_valid, y_train, y_valid):\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \"\"\"\n",
    "    checkpoint = ModelCheckpoint('modelSandBox-{epoch:03d}.h5',\n",
    "                                 monitor='val_loss',\n",
    "                                 verbose=0,\n",
    "                                 save_best_only=psave_best_only,\n",
    "                                 mode='auto')\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=learning_rate), metrics=['accuracy'])\n",
    "\n",
    "    #model.compile(loss='mean_squared_error', optimizer=Adam(lr=learning_rate))\n",
    "    #model.compile(loss='binary_crossentropy', optimizer=Adam(lr=learning_rate))\n",
    "\n",
    "    #model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=learning_rate))\n",
    "    # try to change\n",
    "    #model.compile(loss='mean_squared_error', optimizer=Adam(lr=learning_rate), metrics=['accuracy'])\n",
    "    \n",
    "    model.fit_generator(batch_generator(X_train, y_train, batch_size, True),\n",
    "                        samples_per_epoch,\n",
    "                        nb_epoch,\n",
    "                        max_q_size=1,\n",
    "                        validation_data = batch_generator(X_valid, y_valid, batch_size, False),\n",
    "                        nb_val_samples=len(X_valid),\n",
    "                        callbacks=[checkpoint],\n",
    "                        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model.\n",
    "As a result, files named like **model-000.h5**, **model-003.h5**, and so on will be saved on the project folder.\n",
    "Those files are trainned models that can be used later to classify the gestures.\n",
    "The numbers on their names are meaningfull:\n",
    "At the end of each epoch run, a loss value is obtainned. If it is the first epoch, the file numbered 000 is recorded. If not, if it beats the lower (less is better here!) loss value obtainned in the former epochs, a file with the model will be saved. The numbers are just epoch -1.\n",
    "Thus, by the end of the run, the best model obtainned will be the one with the highest number on it's name.\n",
    "By the other hand, if the value of parameter **psave_best_only** passed to the funcion **train_model** is false, all epochs will be saved. In this case, there is no indication witch was the best result on the files themselves, so, the user have to take note of the run, and observe the orders of losses values picking the lowest as the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "5000/5000 [==============================] - 132s - loss: 1.0122 - acc: 0.6930 - val_loss: 1.6695 - val_acc: 0.4750\n"
     ]
    }
   ],
   "source": [
    "psave_best_only = True\n",
    "learning_rate = 1.0e-4\n",
    "#samples_per_epoch = 20000\n",
    "samples_per_epoch = 5000\n",
    "#nb_epoch = 10\n",
    "nb_epoch = 1\n",
    "batch_size = 40\n",
    "train_model(model, psave_best_only, learning_rate, samples_per_epoch, nb_epoch, batch_size, \n",
    "            X_train, X_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3 - Operate the robotic gripper using gestures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import serial\n",
    "from keras.models import load_model\n",
    "import sandboxUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **start_operation** generates commands and send them to the Arduino board, via USB.\n",
    "It has a logical parameter named **check_predictions_only**, that is false by default. If it is passed with true value, the function does it work jumping all Arduino comunnication. This is just to allow testing the result of a model without the need of assembling the robotic grip part of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startOperation(model, check_predictions_only = False):\n",
    "    if not check_predictions_only:\n",
    "        #start serial\n",
    "        ser = serial.Serial('/dev/ttyACM0', 9600, timeout=1)\n",
    "        print('Serial connection: ', ser.name)\n",
    "    else:\n",
    "        ser = None\n",
    "    # variables to define play warning sound\n",
    "    frequency = 100 # Hertz\n",
    "    duration  = 50 # milliseconds\n",
    "    gc = ' '\n",
    "    #using webcam 0.\n",
    "    #in some systems webcam may be under different numbers, i.e, 1 or 2 or 3 ...\n",
    "    vid = cv2.VideoCapture(0)\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        while True:\n",
    "            # Capture frame-by-frame\n",
    "            ret, frame = vid.read()\n",
    "            if not ret:\n",
    "                # Release the Video Device if ret is false\n",
    "                vid.release()\n",
    "                # Message to be displayed after releasing the device\n",
    "                print(\"Released Video Resource due to capture fail!\")\n",
    "                break\n",
    "            # Convert the image from OpenCV BGR format to matplotlib RGB format\n",
    "            # to display the image\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            # check if it is time to save frame to a file\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if elapsed_time > 2:\n",
    "                # make sound to indicate action\n",
    "                os.system('play -n synth %s sin %s' % (duration/1000, frequency))\n",
    "                # predict\n",
    "                gc = predict(model, ser, frame, check_predictions_only)\n",
    "                #restart the timer\n",
    "                start_time = time.time()\n",
    "            # Turn off the axis\n",
    "            axis('off')\n",
    "            # Title of the window\n",
    "            title(\"Gripper Gesture: (\" + gc + ')')\n",
    "            # Display the frame\n",
    "            imshow(frame)\n",
    "            show()\n",
    "            # Display the frame until new frame is available\n",
    "            clear_output(wait=True)\n",
    "    except KeyboardInterrupt:\n",
    "        # Message to be displayed after releasing the device\n",
    "        print(\"keyboard interrupted!\")\n",
    "    # Release the Video Device\n",
    "    vid.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, ser, frame, check_predictions_only = False):\n",
    "    # The current frame of gesture\n",
    "    gc = ' '\n",
    "    labels = ['nothing', 'left', 'right', 'grip', 'loose', 'foward', 'back', 'up', 'down']\n",
    "    rlabels = ['n', 'l', 'r', 'g', 'o', 'f', 'b', 'u', 'd']\n",
    "    try:\n",
    "        frame = utils.preprocess(frame) # apply the preprocessing\n",
    "        frame = np.array([frame])       # the model expects 4D array\n",
    "        # predict the gesture\n",
    "        #gesture = float(model.predict(frame, batch_size=1))\n",
    "        #as the output layer now is softmax over 9 classes, we must addapt to it\n",
    "        # print('gesture prediction: [', round(gesture), ' <- ', gesture, '] ', labels[round(gesture)])\n",
    "        gesture = np.argmax(model.predict(frame, batch_size=1))\n",
    "        print('gesture prediction: ', gesture)\n",
    "#        if(gesture <= 0.8):\n",
    "#            gc = 'n';\n",
    "#        elif(gesture <= 1.8):\n",
    "#            gc = 'l';\n",
    "#        elif(gesture <= 2.8):\n",
    "#            gc = 'r';\n",
    "#        elif(gesture <= 3.8):\n",
    "#            gc = 'g';\n",
    "#        elif(gesture <= 4.8):\n",
    "#            gc = 'o';\n",
    "#        elif(gesture <= 5.8):\n",
    "#            gc = 'f';\n",
    "#        elif(gesture <= 6.8):\n",
    "#            gc = 'b';\n",
    "#        elif(gesture <= 7.8):\n",
    "#            gc = 'u';\n",
    "#        elif(gesture <= 8.8):\n",
    "#            gc = 'd';\n",
    "\n",
    "        if(gesture != ' '):\n",
    "            #print('gesture: ', gc)\n",
    "            print('gesture: ', rlabels[gesture])\n",
    "            if not check_predictions_only:\n",
    "                #ser.write(bytes(gc, 'utf-8'))\n",
    "                ser.write(bytes(rlabels[gesture], 'utf-8'))\n",
    "                time.sleep(.02)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    #return gc\n",
    "    return rlabels[gesture]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the model we want to use for recognize the gestures. Remember, usually a bigger the number on the name of a model file indicates that it's minimization was better than the previous ones, so you better choose the higher number file to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('modelSandBox-000.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally it is time to run the project, and control the robotic gripper.\n",
    "Due to a limitation on jupyter notebooks IPhython, when a cell is running, we can't interact with it via keyboard.\n",
    "So, to interrupt a running sell one should press **Kernel** on notebook menu, and then press Interrupt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startOperation(model, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra\n",
    "\n",
    "Extra code to run small parts in order to test or understand them better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import serial\n",
    "from keras.models import load_model\n",
    "import sandboxUtils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sandboxUtils import INPUT_SHAPE, batch_generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('modelSandBox-000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    load_images_from_path\n",
    "'''\n",
    "def load_images_from_path(path, result, images, results):\n",
    "    for filename in os.listdir(path):\n",
    "      img = os.path.join(path,filename)\n",
    "      if img is not None:\n",
    "        images.append(img)\n",
    "        results.append(result)\n",
    "    return images, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "  images = []\n",
    "  results =[]\n",
    "  labels = ['nothing', 'left', 'right', 'grip', 'loose', 'foward', 'back', 'up', 'down']\n",
    "\n",
    "  #load a list of images and a corresponding list of results (images=640x480)\n",
    "  images, results = load_images_from_path('sandboxCapture/nothing/', 0, images, results)\n",
    "  images, results = load_images_from_path('sandboxCapture/left/', 1, images, results)\n",
    "  images, results = load_images_from_path('sandboxCapture/right/', 2, images, results)\n",
    "  images, results = load_images_from_path('sandboxCapture/grip/', 3, images, results)\n",
    "  images, results = load_images_from_path('sandboxCapture/loose/', 4, images, results)\n",
    "  images, results = load_images_from_path('sandboxCapture/foward/', 5, images, results)\n",
    "  images, results = load_images_from_path('sandboxCapture/back/', 6, images, results)\n",
    "  images, results = load_images_from_path('sandboxCapture/up/', 7, images, results)\n",
    "  images, results = load_images_from_path('sandboxCapture/down/', 8, images, results)\n",
    "\n",
    "  X_train, X_valid, y_train, y_valid = train_test_split(images, results, test_size=0.2, shuffle = True, random_state=0)\n",
    "\n",
    "  return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Images:  20\n",
      "Validation Results:  20\n"
     ]
    }
   ],
   "source": [
    "#lets load the data\n",
    "__, X_valid, ___, y_valid = load_data()\n",
    "\n",
    "labels = ['nothing', 'left', 'right', 'grip', 'loose', 'foward', 'back', 'up', 'down']\n",
    "\n",
    "print(\"Validation Images: \", len(X_valid))\n",
    "print(\"Validation Results: \", len(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth: 8  :  down  -- prediction:  down ( 8 )\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "idx = random.randint(0, len(X_valid)-1)\n",
    "image = sandboxUtils.load_image(X_valid[idx])\n",
    "image = sandboxUtils.preprocess(image)\n",
    "image = np.array([image])       # the model expects 4D array\n",
    "gesture = np.argmax(model.predict(image, batch_size=1))\n",
    "print('ground truth:', y_valid[idx], ' : ', labels[y_valid[idx]], ' -- prediction: ', labels[gesture], '(', gesture, ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
