{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROBOTIC GRIPPER\n",
    "\n",
    "## A Robotic Gripper Operated by Gestures Learned Trough DeepLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project allows a user to control a robotic gripper using gestures captured by a webcam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - How does it works\n",
    "\n",
    "The project is diveded in 3 main phases, in order to fulfill user requests:\n",
    "\n",
    "    - Phase 1: Images must be captured from the webcam to compound a labeled gestures dataset.\n",
    "    The dataset will feed trainning and testing datasets to be used in supervised learning.\n",
    "    \n",
    "    - Phase 2: A deep learning model, basically a neural network, will be created and used to train the gestures recognition, using keras and tensorflow.\n",
    "    \n",
    "    - Phase 3: A program will be used to sequentially capture webcam images.\n",
    "    The images will be classifyed by the model trainned in Phase 2, and the result will be used to operate the robotic gripper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Capturing labeled gestures images\n",
    "\n",
    "Images will be captured from the webcam.\n",
    "A folder named **capture** will have several subfolders.\n",
    "The subfolders will have meaningful names, such as **left**, **right**, and so on.\n",
    "The subfolder named **left** will hold images of teh gesture that yields the command **turn to the left**.\n",
    "This is so that later the subfolders name will become the ground truth values of the datasets for the machine learning process.\n",
    "\n",
    "For controlling the robotic gripper, we are going to use nine commands:\n",
    "    1. nothing\n",
    "    2. left\n",
    "    3. right\n",
    "    4. up\n",
    "    5. down\n",
    "    6. foward\n",
    "    7. back\n",
    "    8. grip\n",
    "    9. loose\n",
    "    \n",
    "Some examples of images are:\n",
    "\n",
    "<center><table>\n",
    "<TR>\n",
    "  <TD>\n",
    "      <img src=\"images/nothing.jpg\" width=\"128\" height=\"96\" />\n",
    "  </TD>\n",
    "  <TD>\n",
    "      <img src=\"images/left.jpg\" width=\"128\" height=\"96\" />\n",
    "  </TD>\n",
    "  <TD>\n",
    "      <img src=\"images/right.jpg\" width=\"128\" height=\"96\" />\n",
    "  </TD>\n",
    "  <TD>\n",
    "      <img src=\"images/grip.jpg\" width=\"128\" height=\"96\" />\n",
    "  </TD>\n",
    "  <TD>\n",
    "      <img src=\"images/loose.jpg\" width=\"128\" height=\"96\" />\n",
    "  </TD>\n",
    "</TR>\n",
    "<TR>\n",
    "  <TD>\n",
    "      nothing\n",
    "  </TD>\n",
    "  <TD>\n",
    "      left\n",
    "  </TD>\n",
    "  <TD>\n",
    "      right\n",
    "  </TD>\n",
    "  <TD>\n",
    "      grip\n",
    "  </TD>\n",
    "  <TD>\n",
    "      loose\n",
    "  </TD>\n",
    "</TR>\n",
    "</center></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "%pylab inline \n",
    "import cv2\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    function  start_webcam_capture\n",
    "    parameters:\n",
    "    path - the path to save captured gesture images files\n",
    "\"\"\"\n",
    "def start_webcam_capture(path):\n",
    "    # variables to define play warning sound\n",
    "    frequency = 100 # Hertz\n",
    "    duration  = 50 # milliseconds\n",
    "    #lets make sure the path exists!\n",
    "    if not os.access(path, os.F_OK):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    #using webcam 0.\n",
    "    #in some systems webcam may be under different numbers, i.e, 1 or 2 or 3 ...\n",
    "    vid = cv2.VideoCapture(0)\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        while(True):\n",
    "            # Capture frame-by-frame\n",
    "            ret, frame = vid.read()\n",
    "            if not ret:\n",
    "                # Release the Video Device if ret is false\n",
    "                vid.release()\n",
    "                # Message to be displayed after releasing the device\n",
    "                print(\"Released Video Resource due to capture fail!\")\n",
    "                break\n",
    "            # Convert the image from OpenCV BGR format to matplotlib RGB format\n",
    "            # to display the image\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            # check if it is time to save frame to a file\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if elapsed_time > 4:\n",
    "                # make sound to indicate action\n",
    "                os.system('play -n synth %s sin %s' % (duration/1000, frequency))\n",
    "                timestamp = datetime.utcnow().strftime('%Y_%m_%d_%H_%M_%S_%f')[:-3]\n",
    "                timestamp = timestamp + '.jpg'\n",
    "                image_filename = os.path.join(path, timestamp)\n",
    "                #print(image_filename)\n",
    "                cv2.imwrite(image_filename, frame)\n",
    "                #restart the timer\n",
    "                start_time = time.time()\n",
    "            # check for ESC\n",
    "            key = np.int16(cv2.waitKey(1))\n",
    "            if key == 27:\n",
    "                print(\"Esc key interrupted!\")\n",
    "                break  # esc to quit\n",
    "            # Turn off the axis\n",
    "            axis('off')\n",
    "            # Title of the window\n",
    "            title(\"Robotic Gripper Gestures Capture\")\n",
    "            # Display the frame\n",
    "            imshow(frame)\n",
    "            show()\n",
    "            # Display the frame until new frame is available\n",
    "            clear_output(wait=True)\n",
    "    except KeyboardInterrupt:\n",
    "        # Message to be displayed after releasing the device\n",
    "        print(\"keyboard interrupted!\")\n",
    "    # Release the Video Device\n",
    "    vid.release()\n",
    "    print(\"Released Video Resource\")\n",
    "    path, dirs, files = os.walk(path).__next__()\n",
    "    file_count = len(files)\n",
    "    print('There are now ', file_count, ' images in ', path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by capturing the gesture for **nothing**.\n",
    "When you are done, select **Kernel** on jupyter notebook menu and then select **Interrupt**\n",
    "As the file names are bases on a complete and unique timestamp, if you wish, you can run the same code again to add more gestures images. You can even visually select and remove some files (in case of a mistake) using a external file manager from your operating system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyboard interrupted!\n",
      "Released Video Resource\n",
      "There are now  14  images in  capture/nothing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb749662780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = 'capture/nothing'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **left**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'capture/left'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **right**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'capture/right'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **up**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'capture/up'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **down**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'capture/down'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **foward**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'capture/foward'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **back**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'capture/back'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **grip**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'capture/grip'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **loose**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'capture/loose'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Build the Model and train it using the captured gestures from the first phase\n",
    "\n",
    "We are going to build our [deep learning](https://en.wikipedia.org/wiki/Deep_learning) robotic gripper gesture commands model using [Keras](https://keras.io/) and [TensorFlow](https://www.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Lambda, Conv2D, MaxPooling2D, Dropout, Dense, Flatten\n",
    "from utils import INPUT_SHAPE, batch_generator\n",
    "import argparse\n",
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    load_images_from_folder\n",
    "'''\n",
    "def load_images_from_folder(folder, result, images, results):\n",
    "    print('folder: ', folder)\n",
    "    for filename in os.listdir(folder):\n",
    "      img = os.path.join(folder,filename)\n",
    "      if img is not None:\n",
    "        images.append(img)\n",
    "        results.append(result)\n",
    "    return images, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(args):\n",
    "  images = []\n",
    "  results =[]\n",
    "  labels = ['nothing', 'left', 'right', 'grip', 'loose']\n",
    "\n",
    "  #load a list of images and a corresponding list of results (images=640x480)\n",
    "  images, results = load_images_from_folder('capture/nothing01/', 0, images, results)\n",
    "  images, results = load_images_from_folder('capture/left01/', 1, images, results)\n",
    "  images, results = load_images_from_folder('capture/right01/', 2, images, results)\n",
    "  images, results = load_images_from_folder('capture/grip01/', 3, images, results)\n",
    "  images, results = load_images_from_folder('capture/loose01/', 4, images, results)\n",
    "\n",
    "  print(\"Images: \", len(images))\n",
    "  print(\"Results: \", len(results))\n",
    "  print(\"labels: \", len(labels), labels)\n",
    "\n",
    "  # if we wish to check some of the images, just change de index value\n",
    "  # note that the index can't be bigger than the number of images -1\n",
    "  #cv2.imshow('Capture', cv2.imread(images[80]))\n",
    "  #print(images[80])\n",
    "  #print(labels[results[80]])\n",
    "  #cv2.waitKey(0)\n",
    "  #X = np.asarray(images)\n",
    "  #y = np.asarray(results)\n",
    "  #X = X.reshape(len(images),1)\n",
    "  #y = y.reshape(len(results),1)\n",
    "  #print('X shape: ', X.shape)\n",
    "  #print('y shape: ', y.shape)\n",
    "  X_train, X_valid, y_train, y_valid = train_test_split(images, results, test_size=0.2, shuffle = True, random_state=0)\n",
    "\n",
    "  print(\"Train Images: \", len(X_train))\n",
    "  print(\"Valid Images: \", len(X_valid))\n",
    "  print(\"Train Results: \", len(y_train))\n",
    "  print(\"Valid Results: \", len(y_valid))\n",
    "\n",
    "  # if we wish to check some of the images, just change de index value\n",
    "  # note that the index can't be bigger than the number of images -1\n",
    "  #cv2.imshow('Capture', cv2.imread(X_train[80]))\n",
    "  #print(X_train[80])\n",
    "  #print(labels[results[80]])\n",
    "  #cv2.waitKey(0)\n",
    "  #cv2.destroyAllWindows()\n",
    "  #sys.exit(0)\n",
    "\n",
    "  return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(args):\n",
    "    \"\"\"\n",
    "    Modified NVIDIA model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: x/127.5-1.0, input_shape=INPUT_SHAPE))\n",
    "    model.add(Conv2D(24, 5, 5, activation='elu', subsample=(2, 2)))\n",
    "    model.add(Conv2D(36, 5, 5, activation='elu', subsample=(2, 2)))\n",
    "    model.add(Conv2D(48, 5, 5, activation='elu', subsample=(2, 2)))\n",
    "    model.add(Conv2D(64, 3, 3, activation='elu'))\n",
    "    model.add(Conv2D(64, 3, 3, activation='elu'))\n",
    "    model.add(Dropout(args.keep_prob))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='elu'))\n",
    "    model.add(Dense(50, activation='elu'))\n",
    "    model.add(Dense(10, activation='elu'))\n",
    "    model.add(Dense(1))\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, args, X_train, X_valid, y_train, y_valid):\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \"\"\"\n",
    "    checkpoint = ModelCheckpoint('model-{epoch:03d}.h5',\n",
    "                                 monitor='val_loss',\n",
    "                                 verbose=0,\n",
    "                                 save_best_only=args.save_best_only,\n",
    "                                 mode='auto')\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(lr=args.learning_rate))\n",
    "    \n",
    "    model.fit_generator(batch_generator(X_train, y_train, args.batch_size, True),\n",
    "                        args.samples_per_epoch,\n",
    "                        args.nb_epoch,\n",
    "                        max_q_size=1,\n",
    "                        validation_data = batch_generator(X_valid, y_valid, args.batch_size, False),\n",
    "                        nb_val_samples=len(X_valid),\n",
    "                        callbacks=[checkpoint],\n",
    "                        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s2b(s):\n",
    "    \"\"\"\n",
    "    Converts a string to boolean value\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "    return s == 'true' or s == 'yes' or s == 'y' or s == '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Load train/validation data set and train the model\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Behavioral Cloning Training Program')\n",
    "    parser.add_argument('-d', help='capture directory',        dest='capture_dir',          type=str,   default='capture')\n",
    "    parser.add_argument('-t', help='test size fraction',    dest='test_size',         type=float, default=0.2)\n",
    "    parser.add_argument('-k', help='drop out probability',  dest='keep_prob',         type=float, default=0.5)\n",
    "    parser.add_argument('-n', help='number of epochs',      dest='nb_epoch',          type=int,   default=10)\n",
    "    parser.add_argument('-s', help='samples per epoch',     dest='samples_per_epoch', type=int,   default=20000)\n",
    "    parser.add_argument('-b', help='batch size',            dest='batch_size',        type=int,   default=40)\n",
    "    parser.add_argument('-o', help='save best models only', dest='save_best_only',    type=s2b,   default='true')\n",
    "    parser.add_argument('-l', help='learning rate',         dest='learning_rate',     type=float, default=1.0e-4)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print('-' * 30)\n",
    "    print('Parameters')\n",
    "    print('-' * 30)\n",
    "    for key, value in vars(args).items():\n",
    "        print('{:<20} := {}'.format(key, value))\n",
    "    print('-' * 30)\n",
    "\n",
    "    data = load_data(args)\n",
    "    model = build_model(args)\n",
    "    train_model(model, args, *data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
