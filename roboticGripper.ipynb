{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROBOTIC GRIPPER\n",
    "\n",
    "## A Robotic Gripper Operated by Gestures Learned Trough DeepLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project allows a user to control a robotic gripper using gestures captured by a webcam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - How does it works\n",
    "\n",
    "The project is diveded in 3 main phases, in order to fulfill user requests:\n",
    "\n",
    "    - Phase 1: Images must be captured from the webcam to compound a labeled gestures dataset.\n",
    "    The dataset will feed trainning and testing datasets to be used in supervised learning.\n",
    "    \n",
    "    - Phase 2: A deep learning model, basically a neural network, will be created and used to train the gestures recognition, using keras and tensorflow.\n",
    "    \n",
    "    - Phase 3: A program will be used to sequentially capture webcam images.\n",
    "    The images will be classifyed by the model trainned in Phase 2, and the result will be used to operate the robotic gripper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Capturing labeled gestures images\n",
    "\n",
    "Images will be captured from the webcam.\n",
    "A folder named **capture** will have several subfolders.\n",
    "The subfolders will have meaningful names, such as **left**, **right**, and so on.\n",
    "The subfolder named **left** will hold images of teh gesture that yields the command **turn to the left**.\n",
    "This is so that later the subfolders name will become the ground truth values of the datasets for the machine learning process.\n",
    "\n",
    "For controlling the robotic gripper, we are going to use nine commands:\n",
    "    1. nothing\n",
    "    2. left\n",
    "    3. right\n",
    "    4. up\n",
    "    5. down\n",
    "    6. foward\n",
    "    7. back\n",
    "    8. grip\n",
    "    9. loose\n",
    "    \n",
    "Some examples of images are:\n",
    "\n",
    "<TABLE>\n",
    "    <CENTER>\n",
    "    <TR>\n",
    "      <TD>\n",
    "          <img src=\"images/nothing.jpg\" width=\"128\" height=\"96\" />\n",
    "      </TD>\n",
    "      <TD>\n",
    "          <img src=\"images/left.jpg\" width=\"128\" height=\"96\" />\n",
    "      </TD>\n",
    "      <TD>\n",
    "          <img src=\"images/right.jpg\" width=\"128\" height=\"96\" />\n",
    "      </TD>\n",
    "      <TD>\n",
    "          <img src=\"images/grip.jpg\" width=\"128\" height=\"96\" />\n",
    "      </TD>\n",
    "      <TD>\n",
    "          <img src=\"images/loose.jpg\" width=\"128\" height=\"96\" />\n",
    "      </TD>\n",
    "    </TR>\n",
    "    <TR>\n",
    "      <TD>\n",
    "          nothing\n",
    "      </TD>\n",
    "      <TD>\n",
    "          left\n",
    "      </TD>\n",
    "      <TD>\n",
    "          right\n",
    "      </TD>\n",
    "      <TD>\n",
    "          grip\n",
    "      </TD>\n",
    "      <TD>\n",
    "          loose\n",
    "      </TD>\n",
    "    </TR>\n",
    "    <TR>\n",
    "      <TD>\n",
    "          <img src=\"images/foward.jpg\" width=\"128\" height=\"96\" />\n",
    "      </TD>\n",
    "      <TD>\n",
    "          <img src=\"images/back.jpg\" width=\"128\" height=\"96\" />\n",
    "      </TD>\n",
    "      <TD>\n",
    "          <img src=\"images/up.jpg\" width=\"128\" height=\"96\" />\n",
    "      </TD>\n",
    "      <TD>\n",
    "          <img src=\"images/down.jpg\" width=\"128\" height=\"96\" />\n",
    "      </TD>\n",
    "    </TR>\n",
    "    <TR>\n",
    "      <TD>\n",
    "          foward\n",
    "      </TD>\n",
    "      <TD>\n",
    "          back\n",
    "      </TD>\n",
    "      <TD>\n",
    "          up\n",
    "      </TD>\n",
    "      <TD>\n",
    "          down\n",
    "      </TD>\n",
    "    </TR>\n",
    "    </CENTER>\n",
    "</TABLE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "%pylab inline \n",
    "import cv2\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    function  start_webcam_capture\n",
    "    parameters:\n",
    "    path - the path to save captured gesture images files\n",
    "\"\"\"\n",
    "def start_webcam_capture(path, number_of_captures=10):\n",
    "    # variables to define play warning sound\n",
    "    frequency = 100 # Hertz\n",
    "    duration  = 50 # milliseconds\n",
    "    #lets make sure the path exists!\n",
    "    if not os.access(path, os.F_OK):\n",
    "        os.makedirs(path)\n",
    "    count_captures = 0\n",
    "    #using webcam 0.\n",
    "    #in some systems webcam may be under different numbers, i.e, 1 or 2 or 3 ...\n",
    "    vid = cv2.VideoCapture(0)\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        while(count_captures<number_of_captures):\n",
    "            # Capture frame-by-frame\n",
    "            ret, frame = vid.read()\n",
    "            if not ret:\n",
    "                # Release the Video Device if ret is false\n",
    "                vid.release()\n",
    "                # Message to be displayed after releasing the device\n",
    "                print(\"Released Video Resource due to capture fail!\")\n",
    "                break\n",
    "            # Convert the image from OpenCV BGR format to matplotlib RGB format\n",
    "            # to display the image\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            # check if it is time to save frame to a file\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if elapsed_time > 4:\n",
    "                # make sound to indicate action\n",
    "                os.system('play -n synth %s sin %s' % (duration/1000, frequency))\n",
    "                timestamp = datetime.utcnow().strftime('%Y_%m_%d_%H_%M_%S_%f')[:-3]\n",
    "                timestamp = timestamp + '.jpg'\n",
    "                image_filename = os.path.join(path, timestamp)\n",
    "                #print(image_filename)\n",
    "                cv2.imwrite(image_filename, frame)\n",
    "                #increment count_captures\n",
    "                count_captures += 1\n",
    "                #restart the timer\n",
    "                start_time = time.time()\n",
    "            # check for ESC\n",
    "            key = np.int16(cv2.waitKey(1))\n",
    "            if key == 27:\n",
    "                print(\"Esc key interrupted!\")\n",
    "                break  # esc to quit\n",
    "            # Turn off the axis\n",
    "            axis('off')\n",
    "            # Title of the window\n",
    "            title(\"Robotic Gripper Gestures Capture\")\n",
    "            # Display the frame\n",
    "            imshow(frame)\n",
    "            show()\n",
    "            # Display the frame until new frame is available\n",
    "            clear_output(wait=True)\n",
    "    except KeyboardInterrupt:\n",
    "        # Message to be displayed after releasing the device\n",
    "        print(\"keyboard interrupted!\")\n",
    "    # Release the Video Device\n",
    "    vid.release()\n",
    "    print(\"Released Video Resource\")\n",
    "    path, dirs, files = os.walk(path).__next__()\n",
    "    file_count = len(files)\n",
    "    print('There are now ', file_count, ' images in ', path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by capturing the gesture for **nothing**.\n",
    "When you are done, select **Kernel** on jupyter notebook menu and then select **Interrupt**\n",
    "As the file names are bases on a complete and unique timestamp, if you wish, you can run the same code again to add more gestures images. You can even visually select and remove some files (in case of a mistake) using a external file manager from your operating system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'capture/nothing'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **left**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'capture/left'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **right**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Released Video Resource\n",
      "There are now  10  images in  capture/right\n"
     ]
    }
   ],
   "source": [
    "path = 'capture/right'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **up**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Released Video Resource\n",
      "There are now  10  images in  capture/up\n"
     ]
    }
   ],
   "source": [
    "path = 'capture/up'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **down**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Released Video Resource\n",
      "There are now  10  images in  capture/down\n"
     ]
    }
   ],
   "source": [
    "path = 'capture/down'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **foward**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Released Video Resource\n",
      "There are now  10  images in  capture/foward\n"
     ]
    }
   ],
   "source": [
    "path = 'capture/foward'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **back**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Released Video Resource\n",
      "There are now  10  images in  capture/back\n"
     ]
    }
   ],
   "source": [
    "path = 'capture/back'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **grip**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Released Video Resource\n",
      "There are now  10  images in  capture/grip\n"
     ]
    }
   ],
   "source": [
    "path = 'capture/grip'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **loose**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Released Video Resource\n",
      "There are now  10  images in  capture/loose\n"
     ]
    }
   ],
   "source": [
    "path = 'capture/loose'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Build the Model and train it using the captured gestures from the first phase\n",
    "\n",
    "We are going to build our [deep learning](https://en.wikipedia.org/wiki/Deep_learning) robotic gripper gesture commands model using [Keras](https://keras.io/) and [TensorFlow](https://www.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Lambda, Conv2D, MaxPooling2D, Dropout, Dense, Flatten\n",
    "from utils import INPUT_SHAPE, batch_generator\n",
    "import argparse\n",
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **load_images_from_path** is auxiliary to the function **load_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    load_images_from_path\n",
    "'''\n",
    "def load_images_from_path(path, result, images, results):\n",
    "    for filename in os.listdir(path):\n",
    "      img = os.path.join(path,filename)\n",
    "      if img is not None:\n",
    "        images.append(img)\n",
    "        results.append(result)\n",
    "    return images, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "  images = []\n",
    "  results =[]\n",
    "  labels = ['nothing', 'left', 'right', 'grip', 'loose', 'foward', 'back', 'up', 'down']\n",
    "\n",
    "  #load a list of images and a corresponding list of results (images=640x480)\n",
    "  images, results = load_images_from_path('capture/nothing/', 0, images, results)\n",
    "  images, results = load_images_from_path('capture/left/', 1, images, results)\n",
    "  images, results = load_images_from_path('capture/right/', 2, images, results)\n",
    "  images, results = load_images_from_path('capture/grip/', 3, images, results)\n",
    "  images, results = load_images_from_path('capture/loose/', 4, images, results)\n",
    "  images, results = load_images_from_path('capture/foward/', 5, images, results)\n",
    "  images, results = load_images_from_path('capture/back/', 6, images, results)\n",
    "  images, results = load_images_from_path('capture/up/', 7, images, results)\n",
    "  images, results = load_images_from_path('capture/down/', 8, images, results)\n",
    "\n",
    "  X_train, X_valid, y_train, y_valid = train_test_split(images, results, test_size=0.2, shuffle = True, random_state=0)\n",
    "\n",
    "  return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images:  79\n",
      "Valid Images:  20\n",
      "Train Results:  79\n",
      "Valid Results:  20\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = load_data()\n",
    "\n",
    "print(\"Train Images: \", len(X_train))\n",
    "print(\"Valid Images: \", len(X_valid))\n",
    "print(\"Train Results: \", len(y_train))\n",
    "print(\"Valid Results: \", len(y_valid))\n",
    "\n",
    "# if we wish to check some of the images, just change de index value\n",
    "# note that the index can't be bigger than the number of images -1\n",
    "#cv2.imshow('Capture', cv2.imread(X_train[80]))\n",
    "#print(X_train[80])\n",
    "#print(labels[results[80]])\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()\n",
    "#sys.exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(keep_prob):\n",
    "    \"\"\"\n",
    "    Modified NVIDIA model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: x/127.5-1.0, input_shape=INPUT_SHAPE))\n",
    "    model.add(Conv2D(24, 5, 5, activation='elu', subsample=(2, 2)))\n",
    "    model.add(Conv2D(36, 5, 5, activation='elu', subsample=(2, 2)))\n",
    "    model.add(Conv2D(48, 5, 5, activation='elu', subsample=(2, 2)))\n",
    "    model.add(Conv2D(64, 3, 3, activation='elu'))\n",
    "    model.add(Conv2D(64, 3, 3, activation='elu'))\n",
    "    model.add(Dropout(keep_prob))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='elu'))\n",
    "    model.add(Dense(50, activation='elu'))\n",
    "    model.add(Dense(10, activation='elu'))\n",
    "    model.add(Dense(1))\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_2 (Lambda)                (None, 120, 160, 3)   0           lambda_input_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)  (None, 58, 78, 24)    1824        lambda_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_7 (Convolution2D)  (None, 27, 37, 36)    21636       convolution2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_8 (Convolution2D)  (None, 12, 17, 48)    43248       convolution2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_9 (Convolution2D)  (None, 10, 15, 64)    27712       convolution2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 8, 13, 64)     36928       convolution2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 8, 13, 64)     0           convolution2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 6656)          0           dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 100)           665700      flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 50)            5050        dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 10)            510         dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             11          dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 802,619\n",
      "Trainable params: 802,619\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keep_prob = 0.5\n",
    "model = build_model(keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, psave_best_only, learning_rate, samples_per_epoch, nb_epoch, batch_size, X_train, X_valid, y_train, y_valid):\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \"\"\"\n",
    "    checkpoint = ModelCheckpoint('model-{epoch:03d}.h5',\n",
    "                                 monitor='val_loss',\n",
    "                                 verbose=0,\n",
    "                                 save_best_only=psave_best_only,\n",
    "                                 mode='auto')\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(lr=learning_rate))\n",
    "    \n",
    "    model.fit_generator(batch_generator(X_train, y_train, batch_size, True),\n",
    "                        samples_per_epoch,\n",
    "                        nb_epoch,\n",
    "                        max_q_size=1,\n",
    "                        validation_data = batch_generator(X_valid, y_valid, batch_size, False),\n",
    "                        nb_val_samples=len(X_valid),\n",
    "                        callbacks=[checkpoint],\n",
    "                        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 454s - loss: 2.6744 - val_loss: 0.2206\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 454s - loss: 0.2870 - val_loss: 2.9278\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 453s - loss: 0.1437 - val_loss: 0.0942\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 453s - loss: 0.0824 - val_loss: 0.3310\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 454s - loss: 0.0637 - val_loss: 1.8243\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 448s - loss: 0.0480 - val_loss: 1.7545\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 447s - loss: 0.0377 - val_loss: 4.1550\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 444s - loss: 0.0280 - val_loss: 6.2238\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 449s - loss: 0.0233 - val_loss: 5.9612\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 446s - loss: 0.0196 - val_loss: 8.1218\n"
     ]
    }
   ],
   "source": [
    "psave_best_only = True\n",
    "learning_rate = 1.0e-4\n",
    "samples_per_epoch = 20000\n",
    "nb_epoch = 10\n",
    "batch_size = 40\n",
    "train_model(model, psave_best_only, learning_rate, samples_per_epoch, nb_epoch, batch_size, \n",
    "            X_train, X_valid, y_train, y_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
