{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROBOTIC GRIPPER\n",
    "\n",
    "## A Robotic Gripper Operated by Gestures Learned Trough DeepLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project allows a user to control a robotic gripper using gestures captured by a webcam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - How does it works\n",
    "\n",
    "The project is diveded in 3 main phases, in order to fulfill user requests:\n",
    "\n",
    "- Phase 1: Images must be captured from the webcam to compound a labeled gestures dataset.\n",
    "  The dataset will feed trainning and testing datasets to be used in supervised learning.\n",
    "    \n",
    "- Phase 2: A deep learning model, basically a neural network, will be created and used to train the gestures recognition, using keras and tensorflow.\n",
    "    \n",
    "- Phase 3: A program will be used to sequentially capture webcam images.\n",
    "  The images will be classifyed by the model trainned in Phase 2, and the result will be used to operate the robotic gripper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Capturing labeled gestures images\n",
    "\n",
    "Images will be captured from the webcam.\n",
    "A folder named **capture** will have several subfolders.\n",
    "The subfolders will have meaningful names, such as **left**, **right**, and so on.\n",
    "The subfolder named **left** will hold images of teh gesture that yields the command **turn to the left**.\n",
    "This is so that later the subfolders name will become the ground truth values of the datasets for the machine learning process.\n",
    "\n",
    "For controlling the robotic gripper, we are going to use nine commands:\n",
    "    1. nothing\n",
    "    2. left\n",
    "    3. right\n",
    "    4. up\n",
    "    5. down\n",
    "    6. foward\n",
    "    7. back\n",
    "    8. grip\n",
    "    9. loose\n",
    "    \n",
    "Some examples of images are:\n",
    "\n",
    "<TABLE>\n",
    "    <CENTER>\n",
    "    <TR>\n",
    "      <TD>\n",
    "          <img src=\"images/nothing.jpg\" width=\"128\" height=\"96\" />\n",
    "      </TD>\n",
    "      <TD>\n",
    "          <img src=\"images/left.jpg\" width=\"128\" height=\"96\" />\n",
    "      </TD>\n",
    "      <TD>\n",
    "          <img src=\"images/right.jpg\" width=\"128\" height=\"96\" />\n",
    "      </TD>\n",
    "      <TD>\n",
    "          <img src=\"images/grip.jpg\" width=\"128\" height=\"96\" />\n",
    "      </TD>\n",
    "      <TD>\n",
    "          <img src=\"images/loose.jpg\" width=\"128\" height=\"96\" />\n",
    "      </TD>\n",
    "    </TR>\n",
    "    <TR>\n",
    "      <TD>\n",
    "          nothing\n",
    "      </TD>\n",
    "      <TD>\n",
    "          left\n",
    "      </TD>\n",
    "      <TD>\n",
    "          right\n",
    "      </TD>\n",
    "      <TD>\n",
    "          grip\n",
    "      </TD>\n",
    "      <TD>\n",
    "          loose\n",
    "      </TD>\n",
    "    </TR>\n",
    "    <TR>\n",
    "      <TD>\n",
    "          <img src=\"images/foward.jpg\" width=\"128\" height=\"96\" />\n",
    "      </TD>\n",
    "      <TD>\n",
    "          <img src=\"images/back.jpg\" width=\"128\" height=\"96\" />\n",
    "      </TD>\n",
    "      <TD>\n",
    "          <img src=\"images/up.jpg\" width=\"128\" height=\"96\" />\n",
    "      </TD>\n",
    "      <TD>\n",
    "          <img src=\"images/down.jpg\" width=\"128\" height=\"96\" />\n",
    "      </TD>\n",
    "    </TR>\n",
    "    <TR>\n",
    "      <TD>\n",
    "          foward\n",
    "      </TD>\n",
    "      <TD>\n",
    "          back\n",
    "      </TD>\n",
    "      <TD>\n",
    "          up\n",
    "      </TD>\n",
    "      <TD>\n",
    "          down\n",
    "      </TD>\n",
    "    </TR>\n",
    "    </CENTER>\n",
    "</TABLE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "%pylab inline \n",
    "import cv2\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    function  start_webcam_capture\n",
    "    parameters:\n",
    "    path - the path to save captured gesture images files\n",
    "\"\"\"\n",
    "def start_webcam_capture(path, number_of_captures=10):\n",
    "    # variables to define play warning sound\n",
    "    frequency = 100 # Hertz\n",
    "    duration  = 50 # milliseconds\n",
    "    #lets make sure the path exists!\n",
    "    if not os.access(path, os.F_OK):\n",
    "        os.makedirs(path)\n",
    "    count_captures = 0\n",
    "    #using webcam 0.\n",
    "    #in some systems webcam may be under different numbers, i.e, 1 or 2 or 3 ...\n",
    "    vid = cv2.VideoCapture(0)\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        while(count_captures<number_of_captures):\n",
    "            # Capture frame-by-frame\n",
    "            ret, frame = vid.read()\n",
    "            if not ret:\n",
    "                # Release the Video Device if ret is false\n",
    "                vid.release()\n",
    "                # Message to be displayed after releasing the device\n",
    "                print(\"Released Video Resource due to capture fail!\")\n",
    "                break\n",
    "            # Convert the image from OpenCV BGR format to matplotlib RGB format\n",
    "            # to display the image\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            # check if it is time to save frame to a file\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if elapsed_time > 4:\n",
    "                # make sound to indicate action\n",
    "                os.system('play -n synth %s sin %s' % (duration/1000, frequency))\n",
    "                timestamp = datetime.utcnow().strftime('%Y_%m_%d_%H_%M_%S_%f')[:-3]\n",
    "                timestamp = timestamp + '.jpg'\n",
    "                image_filename = os.path.join(path, timestamp)\n",
    "                #print(image_filename)\n",
    "                cv2.imwrite(image_filename, frame)\n",
    "                #increment count_captures\n",
    "                count_captures += 1\n",
    "                #restart the timer\n",
    "                start_time = time.time()\n",
    "            # check for ESC\n",
    "            key = np.int16(cv2.waitKey(1))\n",
    "            if key == 27:\n",
    "                print(\"Esc key interrupted!\")\n",
    "                break  # esc to quit\n",
    "            # Turn off the axis\n",
    "            axis('off')\n",
    "            # Title of the window\n",
    "            title(\"Robotic Gripper Gestures Capture\")\n",
    "            # Display the frame\n",
    "            imshow(frame)\n",
    "            show()\n",
    "            # Display the frame until new frame is available\n",
    "            clear_output(wait=True)\n",
    "    except KeyboardInterrupt:\n",
    "        # Message to be displayed after releasing the device\n",
    "        print(\"keyboard interrupted!\")\n",
    "    # Release the Video Device\n",
    "    vid.release()\n",
    "    print(\"Released Video Resource\")\n",
    "    path, dirs, files = os.walk(path).__next__()\n",
    "    file_count = len(files)\n",
    "    print('There are now ', file_count, ' images in ', path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by capturing the gesture for **nothing**.\n",
    "When you are done, select **Kernel** on jupyter notebook menu and then select **Interrupt**\n",
    "As the file names are bases on a complete and unique timestamp, if you wish, you can run the same code again to add more gestures images. You can even visually select and remove some files (in case of a mistake) using a external file manager from your operating system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'capture/nothing'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **left**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'capture/left'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **right**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'capture/right'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **up**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'capture/up'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **down**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'capture/down'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **foward**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'capture/foward'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **back**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'capture/back'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **grip**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'capture/grip'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture te gesture for **loose**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'capture/loose'\n",
    "#start capturing gesture images\n",
    "start_webcam_capture(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Build the Model and train it using the captured gestures from the first phase\n",
    "\n",
    "We are going to build our [deep learning](https://en.wikipedia.org/wiki/Deep_learning) robotic gripper gesture commands model using [Keras](https://keras.io/) and [TensorFlow](https://www.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Lambda, Conv2D, MaxPooling2D, Dropout, Dense, Flatten\n",
    "from utils import INPUT_SHAPE, batch_generator\n",
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **load_images_from_path** is auxiliary to the function **load_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    load_images_from_path\n",
    "'''\n",
    "def load_images_from_path(path, result, images, results):\n",
    "    for filename in os.listdir(path):\n",
    "      img = os.path.join(path,filename)\n",
    "      if img is not None:\n",
    "        images.append(img)\n",
    "        results.append(result)\n",
    "    return images, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "  images = []\n",
    "  results =[]\n",
    "  labels = ['nothing', 'left', 'right', 'grip', 'loose', 'foward', 'back', 'up', 'down']\n",
    "\n",
    "  #load a list of images and a corresponding list of results (images=640x480)\n",
    "  images, results = load_images_from_path('capture/nothing/', 0, images, results)\n",
    "  images, results = load_images_from_path('capture/left/', 1, images, results)\n",
    "  images, results = load_images_from_path('capture/right/', 2, images, results)\n",
    "  images, results = load_images_from_path('capture/grip/', 3, images, results)\n",
    "  images, results = load_images_from_path('capture/loose/', 4, images, results)\n",
    "  images, results = load_images_from_path('capture/foward/', 5, images, results)\n",
    "  images, results = load_images_from_path('capture/back/', 6, images, results)\n",
    "  images, results = load_images_from_path('capture/up/', 7, images, results)\n",
    "  images, results = load_images_from_path('capture/down/', 8, images, results)\n",
    "\n",
    "  X_train, X_valid, y_train, y_valid = train_test_split(images, results, test_size=0.2, shuffle = True, random_state=0)\n",
    "\n",
    "  return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = load_data()\n",
    "\n",
    "print(\"Train Images: \", len(X_train))\n",
    "print(\"Valid Images: \", len(X_valid))\n",
    "print(\"Train Results: \", len(y_train))\n",
    "print(\"Valid Results: \", len(y_valid))\n",
    "\n",
    "# if we wish to check some of the images, just change de index value\n",
    "# note that the index can't be bigger than the number of images -1\n",
    "#cv2.imshow('Capture', cv2.imread(X_train[80]))\n",
    "#print(X_train[80])\n",
    "#print(labels[results[80]])\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()\n",
    "#sys.exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(keep_prob):\n",
    "    \"\"\"\n",
    "    Modified NVIDIA model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: x/127.5-1.0, input_shape=INPUT_SHAPE))\n",
    "    model.add(Conv2D(24, 5, 5, activation='elu', subsample=(2, 2)))\n",
    "    model.add(Conv2D(36, 5, 5, activation='elu', subsample=(2, 2)))\n",
    "    model.add(Conv2D(48, 5, 5, activation='elu', subsample=(2, 2)))\n",
    "    model.add(Conv2D(64, 3, 3, activation='elu'))\n",
    "    model.add(Conv2D(64, 3, 3, activation='elu'))\n",
    "    model.add(Dropout(keep_prob))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='elu'))\n",
    "    model.add(Dense(50, activation='elu'))\n",
    "    model.add(Dense(10, activation='elu'))\n",
    "    model.add(Dense(1))\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = 0.5\n",
    "model = build_model(keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, psave_best_only, learning_rate, samples_per_epoch, nb_epoch, batch_size, X_train, X_valid, y_train, y_valid):\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \"\"\"\n",
    "    checkpoint = ModelCheckpoint('model-{epoch:03d}.h5',\n",
    "                                 monitor='val_loss',\n",
    "                                 verbose=0,\n",
    "                                 save_best_only=psave_best_only,\n",
    "                                 mode='auto')\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(lr=learning_rate))\n",
    "    \n",
    "    model.fit_generator(batch_generator(X_train, y_train, batch_size, True),\n",
    "                        samples_per_epoch,\n",
    "                        nb_epoch,\n",
    "                        max_q_size=1,\n",
    "                        validation_data = batch_generator(X_valid, y_valid, batch_size, False),\n",
    "                        nb_val_samples=len(X_valid),\n",
    "                        callbacks=[checkpoint],\n",
    "                        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model.\n",
    "As a result, files named like **model-000.h5**, **model-003.h5**, and so on will be saved on the project folder.\n",
    "Those files are trainned models that can be used later to classify the gestures.\n",
    "The numbers on their names are meaningfull:\n",
    "At the end of each epoch run, a loss value is obtainned. If it is the first epoch, the file numbered 000 is recorded. If not, if it beats the lower (less is better here!) loss value obtainned in the former epochs, a file with the model will be saved. The numbers are just epoch -1.\n",
    "Thus, by the end of the run, the best model obtainned will be the one with the highest number on it's name.\n",
    "By the other hand, if the value of parameter **psave_best_only** passed to the funcion **train_model** is false, all epochs will be saved. In this case, there is no indication witch was the best result on the files themselves, so, the user have to take note of the run, and observe the orders of losses values picking the lowest as the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psave_best_only = True\n",
    "learning_rate = 1.0e-4\n",
    "samples_per_epoch = 20000\n",
    "nb_epoch = 10\n",
    "batch_size = 40\n",
    "train_model(model, psave_best_only, learning_rate, samples_per_epoch, nb_epoch, batch_size, \n",
    "            X_train, X_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3 - Operate the robotic gripper using gestures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import serial\n",
    "from keras.models import load_model\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **start_operation** generates commands and send them to the Arduino board, via USB.\n",
    "It has a logical parameter named **check_predictions_only**, that is false by default. If it is passed with true value, the function does it work jumping all Arduino comunnication. This is just to allow testing the result of a model without the need of assembling the robotic grip part of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startOperation(model, check_predictions_only = False):\n",
    "    if not check_predictions_only:\n",
    "        #start serial\n",
    "        ser = serial.Serial('/dev/ttyACM0', 9600, timeout=1)\n",
    "        print('Serial connection: ', ser.name)\n",
    "    else:\n",
    "        ser = None\n",
    "    # variables to define play warning sound\n",
    "    frequency = 100 # Hertz\n",
    "    duration  = 50 # milliseconds\n",
    "    gc = ' '\n",
    "    #using webcam 0.\n",
    "    #in some systems webcam may be under different numbers, i.e, 1 or 2 or 3 ...\n",
    "    vid = cv2.VideoCapture(0)\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        while True:\n",
    "            # Capture frame-by-frame\n",
    "            ret, frame = vid.read()\n",
    "            if not ret:\n",
    "                # Release the Video Device if ret is false\n",
    "                vid.release()\n",
    "                # Message to be displayed after releasing the device\n",
    "                print(\"Released Video Resource due to capture fail!\")\n",
    "                break\n",
    "            # Convert the image from OpenCV BGR format to matplotlib RGB format\n",
    "            # to display the image\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            # check if it is time to save frame to a file\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if elapsed_time > 2:\n",
    "                # make sound to indicate action\n",
    "                os.system('play -n synth %s sin %s' % (duration/1000, frequency))\n",
    "                # predict\n",
    "                gc = predict(model, ser, frame, check_predictions_only)\n",
    "                #restart the timer\n",
    "                start_time = time.time()\n",
    "            # Turn off the axis\n",
    "            axis('off')\n",
    "            # Title of the window\n",
    "            title(\"Gripper Gesture: (\" + gc + ')')\n",
    "            # Display the frame\n",
    "            imshow(frame)\n",
    "            show()\n",
    "            # Display the frame until new frame is available\n",
    "            clear_output(wait=True)\n",
    "    except KeyboardInterrupt:\n",
    "        # Message to be displayed after releasing the device\n",
    "        print(\"keyboard interrupted!\")\n",
    "    # Release the Video Device\n",
    "    vid.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, ser, frame, check_predictions_only = False):\n",
    "    # The current frame of gesture\n",
    "    gc = ' '\n",
    "    labels = ['nothing', 'left', 'right', 'grip', 'loose', 'foward', 'back', 'up', 'down']\n",
    "\n",
    "    try:\n",
    "        frame = utils.preprocess(frame) # apply the preprocessing\n",
    "        frame = np.array([frame])       # the model expects 4D array\n",
    "        # predict the gesture\n",
    "        gesture = float(model.predict(frame, batch_size=1))\n",
    "        print('gesture prediction: [', round(gesture), ' <- ', gesture, '] ', labels[round(gesture)])\n",
    "        if(gesture <= 0.8):\n",
    "            gc = 'n';\n",
    "        elif(gesture <= 1.8):\n",
    "            gc = 'l';\n",
    "        elif(gesture <= 2.8):\n",
    "            gc = 'r';\n",
    "        elif(gesture <= 3.8):\n",
    "            gc = 'g';\n",
    "        elif(gesture <= 4.8):\n",
    "            gc = 'o';\n",
    "        elif(gesture <= 5.8):\n",
    "            gc = 'f';\n",
    "        elif(gesture <= 6.8):\n",
    "            gc = 'b';\n",
    "        elif(gesture <= 7.8):\n",
    "            gc = 'u';\n",
    "        elif(gesture <= 8.8):\n",
    "            gc = 'd';\n",
    "        if(gesture != ' '):\n",
    "            print('gesture: ', gc)\n",
    "            if not check_predictions_only:\n",
    "                ser.write(bytes(gc, 'utf-8'))\n",
    "                time.sleep(.02)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the model we want to use for recognize the gestures. Remember, usually a bigger the number on the name of a model file indicates that it's minimization was better than the previous ones, so you better choose the higher number file to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model-002.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally it is time to run the project, and control the robotic gripper.\n",
    "Due to a limitation on jupyter notebooks IPhython, when a cell is running, we can't interact with it via keyboard.\n",
    "So, to interrupt a running sell one should press **Kernel** on notebook menu, and then press Interrupt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyboard interrupted!\n"
     ]
    }
   ],
   "source": [
    "startOperation(model, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
